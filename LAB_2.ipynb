{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Questions\n",
        "\n",
        "1. Averaging the validation accuracy across multiple splits can give more consistent results\n",
        ". This is because it reduces the impact of random variations in the data. By averaging over multiple splits, the results become more stable and less dependent on the specific choice of the training and validation sets.\n",
        "\n",
        "2. Averaging the validation accuracy across multiple splits can give a more accurate estimate of test accuracy\n",
        ". This is because it provides a better estimate of the model's generalization performance. By testing the model on multiple validation sets, we get a better sense of how well it will perform on new, unseen data.\n",
        "\n",
        "3. The number of iterations can have an effect on the estimate of test accuracy\n",
        ". In general, increasing the number of iterations can lead to a more accurate estimate. However, there is a trade-off between accuracy and computational cost. As the number of iterations increases, the training time also increases. Therefore, it is important to find a balance between accuracy and computational efficiency.\n",
        "\n",
        "4. While increasing the number of iterations can improve the accuracy of the model, it may not be the best way to deal with a very small train or validation dataset\n",
        ". In such cases, it may be better to increase the size of the dataset by collecting more data or using data augmentation techniques. This can help to improve the model's performance without relying solely on increasing the number of iterations.\n",
        "\n"
      ],
      "metadata": {
        "id": "qqZf75zC3MXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAlLIqgW2_IT",
        "outputId": "b0f01f39-8d3b-4de8-d16d-6075bf76c519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: Accuracy = 0.93\n",
            "Fold 2: Accuracy = 0.91\n",
            "Fold 3: Accuracy = 0.90\n",
            "Fold 4: Accuracy = 0.91\n",
            "Fold 5: Accuracy = 0.85\n",
            "Average Accuracy: 0.90\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "k = 5\n",
        "scores = cross_val_score(clf, X, y, cv=k, scoring='accuracy')\n",
        "for i, score in enumerate(scores):\n",
        "    print(f'Fold {i+1}: Accuracy = {score:.2f}')\n",
        "avg_accuracy = scores.mean()\n",
        "print(f'Average Accuracy: {avg_accuracy:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2-raRiXU3LcM"
      }
    }
  ]
}